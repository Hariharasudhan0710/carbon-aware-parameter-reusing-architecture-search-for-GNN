{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-22T07:24:28.799241Z","iopub.execute_input":"2025-09-22T07:24:28.799886Z","iopub.status.idle":"2025-09-22T07:24:28.804345Z","shell.execute_reply.started":"2025-09-22T07:24:28.799854Z","shell.execute_reply":"2025-09-22T07:24:28.803758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install codecarbon\n!pip install pandas\n!pip install scikit-learn\n!pip install tqdm\n!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n!pip install torch-geometric\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport random\nimport pandas as pd\nfrom torch_geometric.datasets import Amazon\nfrom torch_geometric.nn import GCNConv, GATConv, SAGEConv, BatchNorm, LayerNorm\nfrom codecarbon import EmissionsTracker\n\ndef set_seed(seed=42):\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n# --- Optimized Mask Split ---\ndef create_masks(data, num_train=8000, num_val=4000):\n    num_nodes = data.num_nodes\n    perm = torch.randperm(num_nodes)\n    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n    train_mask[perm[:num_train]] = True\n    val_mask[perm[num_train:num_train+num_val]] = True\n    test_mask[perm[num_train+num_val:]] = True\n    data.train_mask = train_mask\n    data.val_mask = val_mask\n    data.test_mask = test_mask\n    return data\n\nclass GNNSearchSpace:\n    def __init__(self):\n        self.conv_types = ['GCN', 'GAT', 'SAGE']\n        self.norm_types = ['batch', 'layer', 'none']\n        self.activation_types = ['relu', 'elu']\n        self.hidden_dims = [64, 128, 256]\n        self.num_layers = [2, 3]\n        self.dropout_rates = [0.5]\n\n    def sample_architecture(self):\n        return {\n            'conv_type': random.choice(self.conv_types),\n            'activation': random.choice(self.activation_types),\n            'norm_type': random.choice(self.norm_types),\n            'hidden_dim': random.choice(self.hidden_dims),\n            'num_layers': random.choice(self.num_layers),\n            'dropout': random.choice(self.dropout_rates)\n        }\n\nclass GNNModel(nn.Module):\n    def __init__(self, config, input_dim, output_dim):\n        super().__init__()\n        self.num_layers = config['num_layers']\n        self.config = config\n        self.convs = nn.ModuleList()\n        self.norms = nn.ModuleList()\n        in_dim = input_dim\n        hidden_dim = config['hidden_dim']\n        for i in range(self.num_layers):\n            out_dim = hidden_dim if i < self.num_layers - 1 else output_dim\n            if config['conv_type'] == 'GCN':\n                conv = GCNConv(in_dim, out_dim)\n            elif config['conv_type'] == 'GAT':\n                conv = GATConv(in_dim, out_dim, heads=1)\n            elif config['conv_type'] == 'SAGE':\n                conv = SAGEConv(in_dim, out_dim)\n            self.convs.append(conv)\n            if i < self.num_layers - 1:\n                if config['norm_type'] == 'batch':\n                    norm = BatchNorm(out_dim)\n                elif config['norm_type'] == 'layer':\n                    norm = LayerNorm(out_dim)\n                else:\n                    norm = nn.Identity()\n                self.norms.append(norm)\n            in_dim = out_dim\n\n    def forward(self, x, edge_index):\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n            if i < self.num_layers - 1:\n                x = self.norms[i](x)\n                if self.config['activation'] == 'relu':\n                    x = F.relu(x)\n                elif self.config['activation'] == 'elu':\n                    x = F.elu(x)\n                x = F.dropout(x, p=self.config['dropout'], training=self.training)\n        return x\n\nclass CarbonAwareGNNNAS:\n    def __init__(self, dataset_name='computers', population_size=12, generations=5, seed=42,\n                 max_attempts=100, proxy_acc_threshold=0.75, max_proxy_epochs_first_gen=30):\n        set_seed(seed)\n        self.dataset_name = dataset_name\n        self.population_size = population_size\n        self.generations = generations\n        self.search_space = GNNSearchSpace()\n        self.output_file = f\"emissions_{self.dataset_name.lower()}.csv\"\n        self.max_attempts = max_attempts\n        self.proxy_acc_threshold = proxy_acc_threshold\n        self.max_proxy_epochs_first_gen = max_proxy_epochs_first_gen\n        self.load_dataset()\n        self.results = []\n        self.best_architecture = None\n        self.best_score = 0.0\n\n    def load_dataset(self):\n        dataset = Amazon(root=f'data/{self.dataset_name}', name=self.dataset_name)\n        data = dataset[0]\n        self.data = create_masks(data, num_train=8000, num_val=4000)\n        self.input_dim = dataset.num_node_features\n        self.output_dim = dataset.num_classes\n        print(f\"Dataset: Amazon {self.dataset_name}, features: {self.input_dim}, classes: {self.output_dim}\")\n        # Print mask sizes to confirm\n        print(\"Train:\", self.data.train_mask.sum().item(), \n              \"Val:\", self.data.val_mask.sum().item(), \n              \"Test:\", self.data.test_mask.sum().item())\n\n    def calculate_block_reuse(self, arch1, arch2):\n        reuse_score = 0.0\n        total = len(arch1)\n        for k in arch1.keys():\n            if arch1[k] == arch2[k]:\n                reuse_score += 1\n        return reuse_score / total\n\n    def proxy_train_and_eval(self, config, max_epochs=None, acc_threshold=None, check_every=5, min_epochs=5):\n        if max_epochs is None:\n            max_epochs = self.max_proxy_epochs_first_gen\n        if acc_threshold is None:\n            acc_threshold = self.proxy_acc_threshold\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model = GNNModel(config, self.input_dim, self.output_dim).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n        criterion = nn.CrossEntropyLoss()\n        tracker = EmissionsTracker(\n            project_name=f\"gnn_nas_proxy_amazon_{self.dataset_name}\",\n            measure_power_secs=15,\n            output_file=self.output_file,\n            log_level='error'\n        )\n        tracker.start()\n        best_val_acc = 0.0\n        data = self.data.to(device)\n        for epoch in range(max_epochs):\n            model.train()\n            out = model(data.x, data.edge_index)\n            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            model.eval()\n            with torch.no_grad():\n                logits = model(data.x, data.edge_index)\n                val_pred = logits[data.val_mask].argmax(dim=1)\n                val_acc = (val_pred == data.y[data.val_mask]).float().mean().item()\n                best_val_acc = max(best_val_acc, val_acc)\n            if epoch >= min_epochs and (best_val_acc >= acc_threshold):\n                break\n        emissions = tracker.stop()\n        return {'accuracy': best_val_acc, 'carbon': emissions, 'model': model}\n\n    def train_and_evaluate(self, config, epochs=100):\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model = GNNModel(config, self.input_dim, self.output_dim).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n        criterion = nn.CrossEntropyLoss()\n        tracker = EmissionsTracker(\n            project_name=f\"gnn_nas_amazon_{self.dataset_name}\",\n            measure_power_secs=15,\n            output_file=self.output_file,\n            log_level='error'\n        )\n        tracker.start()\n        data = self.data.to(device)\n        best_val_acc = 0\n        for epoch in range(epochs):\n            model.train()\n            out = model(data.x, data.edge_index)\n            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            model.eval()\n            with torch.no_grad():\n                logits = model(data.x, data.edge_index)\n                val_pred = logits[data.val_mask].argmax(dim=1)\n                val_acc = (val_pred == data.y[data.val_mask]).float().mean().item()\n                best_val_acc = max(best_val_acc, val_acc)\n        emissions = tracker.stop()\n        return {'accuracy': best_val_acc, 'carbon': emissions, 'model': model}\n\n    def run_search(self):\n        print(f\"Starting Carbon-Aware GNN NAS on Amazon {self.dataset_name}...\")\n        baseline_config = {\n            'conv_type': 'GCN', 'activation': 'relu', 'norm_type': 'batch',\n            'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.5\n        }\n        self.baseline_config = baseline_config\n        print(\"Training baseline...\")\n        baseline_result = self.train_and_evaluate(baseline_config, epochs=100)\n        print(f\"Baseline - Val Accuracy: {baseline_result['accuracy']:.4f}, Carbon: {baseline_result['carbon']}\")\n        self.best_score = baseline_result['accuracy']\n        self.best_architecture = baseline_config\n\n        for generation in range(self.generations):\n            print(f\"\\nGeneration {generation + 1}/{self.generations}\")\n            population = []\n            attempts = 0\n            while len(population) < self.population_size and attempts < self.max_attempts:\n                config = self.search_space.sample_architecture()\n                result = self.proxy_train_and_eval(config)\n                if result['accuracy'] >= self.proxy_acc_threshold:\n                    population.append({\n                        'config': config,\n                        'proxy_acc': result['accuracy'],\n                        'carbon': result['carbon'],\n                        'model': result['model']\n                    })\n                    print(f\"  Found candidate {len(population)}: Proxy Acc={result['accuracy']:.4f}, Proxy Carbon={result['carbon']}\")\n                attempts += 1\n\n            if not population:\n                print(\"  No viable candidates in this generation. Skipping evolution.\")\n                continue\n\n            final_population = []\n            for idx, candidate in enumerate(population, 1):\n                full_result = self.train_and_evaluate(candidate['config'], epochs=100)\n                final_population.append({\n                    'config': candidate['config'],\n                    'accuracy': full_result['accuracy'],\n                    'carbon': full_result['carbon'],\n                    'model': full_result['model']\n                })\n                print(f\"    Final candidate {idx}: Val Acc={full_result['accuracy']:.4f}, Carbon={full_result['carbon']}\")\n            final_population.sort(key=lambda x: x['accuracy'], reverse=True)\n\n            if final_population:\n                best_candidate = final_population[0]\n                if best_candidate['accuracy'] > self.best_score:\n                    self.best_score = best_candidate['accuracy']\n                    self.best_architecture = best_candidate['config']\n\n        print(\"\\nRetraining best architecture on train + val, and testing...\")\n        train_val_mask = self.data.train_mask | self.data.val_mask\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        data = self.data.to(device)\n        model = GNNModel(self.best_architecture, self.input_dim, self.output_dim).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n        criterion = nn.CrossEntropyLoss()\n        model.train()\n        for epoch in range(100):\n            out = model(data.x, data.edge_index)\n            loss = criterion(out[train_val_mask], data.y[train_val_mask])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        model.eval()\n        with torch.no_grad():\n            test_pred = model(data.x, data.edge_index)[data.test_mask].argmax(dim=1)\n            test_acc = (test_pred == data.y[data.test_mask]).float().mean().item()\n        reuse = self.calculate_block_reuse(self.best_architecture, self.baseline_config)\n        print(\"\\n--- Final Results ---\")\n        print(\"Best Architecture:\", self.best_architecture)\n        print(\"Test Accuracy:\", round(test_acc, 4))\n        print(\"Block Reuse Score:\", round(reuse, 3))\n\nif __name__ == '__main__':\n    nas = CarbonAwareGNNNAS(\n        dataset_name='computers',\n        population_size=12,\n        generations=5,\n        max_attempts=100,\n        proxy_acc_threshold=0.75,\n        max_proxy_epochs_first_gen=30\n    )\n    nas.run_search()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:48:41.187676Z","iopub.execute_input":"2025-09-22T13:48:41.188541Z","iopub.status.idle":"2025-09-22T14:14:41.734623Z","shell.execute_reply.started":"2025-09-22T13:48:41.188509Z","shell.execute_reply":"2025-09-22T14:14:41.733898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport urllib.request\n\nurl = \"https://snap.stanford.edu/data/soc-Epinions1.txt.gz\"\nlocal_gz = \"soc-Epinions1.txt.gz\"\nlocal_txt = \"soc-Epinions1.txt\"\n\nif not os.path.exists(local_txt):\n    print(\"Downloading soc-Epinions1.txt.gz...\")\n    urllib.request.urlretrieve(url, local_gz)\n    print(\"Extracting...\")\n    import gzip, shutil\n    with gzip.open(local_gz, 'rb') as f_in:\n        with open(local_txt, 'wb') as f_out:\n            shutil.copyfileobj(f_in, f_out)\n    print(\"File ready:\", local_txt)\nelse:\n    print(\"File already exists:\", local_txt)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:22:10.077401Z","iopub.execute_input":"2025-09-22T14:22:10.077689Z","iopub.status.idle":"2025-09-22T14:22:11.908108Z","shell.execute_reply.started":"2025-09-22T14:22:10.077670Z","shell.execute_reply":"2025-09-22T14:22:11.907318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport urllib.request\nimport gzip\nimport shutil\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport random\nimport pandas as pd\nfrom sklearn.metrics import f1_score\nimport networkx as nx\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GCNConv, SAGEConv, GATConv\nfrom codecarbon import EmissionsTracker\n\ndef set_seed(seed=42):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n# --- AUTOMATIC DOWNLOAD ---\nurl = \"https://snap.stanford.edu/data/soc-Epinions1.txt.gz\"\nlocal_gz = \"soc-Epinions1.txt.gz\"\nlocal_txt = \"soc-Epinions1.txt\"\nif not os.path.exists(local_txt):\n    print(\"Downloading soc-Epinions1.txt.gz...\")\n    urllib.request.urlretrieve(url, local_gz)\n    print(\"Extracting...\")\n    with gzip.open(local_gz, 'rb') as f_in:\n        with open(local_txt, 'wb') as f_out:\n            shutil.copyfileobj(f_in, f_out)\n    print(\"File ready:\", local_txt)\nelse:\n    print(\"File already exists:\", local_txt)\n\n# --- Robust Loader: Ignore malformed lines and build complete node list ---\ndef load_epinions_graph(path):\n    edge_list = []\n    labels = []\n    nodes = set()\n    with open(path, 'r') as f:\n        for line in f:\n            if line.startswith('#') or not line.strip():\n                continue\n            tokens = line.strip().split()\n            if len(tokens) != 3:\n                continue  # skip lines that do not have exactly 3 tokens\n            u, v, w = map(int, tokens)\n            edge_list.append((u, v))\n            labels.append(1 if w > 0 else 0)\n            nodes.update([u, v])\n    G = nx.DiGraph()\n    G.add_nodes_from(sorted(nodes))  # ensure all nodes present\n    G.add_edges_from(edge_list)\n    return G, labels\n\ndef pyg_data_from_nx(G, labels):\n    node_mapping = dict((n, i) for i, n in enumerate(sorted(G.nodes())))\n    edge_index = torch.tensor([(node_mapping[u], node_mapping[v]) for u, v in G.edges()], dtype=torch.long).t()\n    x = torch.ones((G.number_of_nodes(), 8))  # Dummy node features\n    y = torch.tensor(labels, dtype=torch.long)\n    data = Data(x=x, edge_index=edge_index, edge_y=y)\n    return data\n\ndef create_edge_splits(data, seed=42):\n    np.random.seed(seed)\n    edge_count = data.edge_index.size(1)\n    assert edge_count == data.edge_y.size(0)\n    perm = np.random.permutation(edge_count)\n    num_train = int(0.7 * edge_count)\n    num_val = int(0.1 * edge_count)\n    train_idx = perm[:num_train]\n    val_idx = perm[num_train:num_train+num_val]\n    test_idx = perm[num_train+num_val:]\n    data.train_edge_mask = torch.zeros(edge_count, dtype=torch.bool)\n    data.val_edge_mask = torch.zeros(edge_count, dtype=torch.bool)\n    data.test_edge_mask = torch.zeros(edge_count, dtype=torch.bool)\n    data.train_edge_mask[train_idx] = True\n    data.val_edge_mask[val_idx] = True\n    data.test_edge_mask[test_idx] = True\n    return data\n\nclass GNNSearchSpace:\n    def __init__(self):\n        self.conv_types = ['GCN', 'SAGE', 'GAT']\n        self.hidden_dims = [32, 64, 128]\n        self.num_layers = [2, 3]\n        self.activation_types = ['relu', 'elu']\n        self.dropout_rates = [0.2, 0.5]\n    def sample_architecture(self):\n        return {\n            'conv_type': random.choice(self.conv_types),\n            'hidden_dim': random.choice(self.hidden_dims),\n            'num_layers': random.choice(self.num_layers),\n            'activation': random.choice(self.activation_types),\n            'dropout': random.choice(self.dropout_rates),\n        }\n\nclass EdgeScoreModel(nn.Module):\n    def __init__(self, config, input_dim):\n        super().__init__()\n        conv_map = {'GCN': GCNConv, 'SAGE': SAGEConv, 'GAT': GATConv}\n        self.convs = nn.ModuleList()\n        in_dim = input_dim\n        for i in range(config['num_layers']):\n            out_dim = config['hidden_dim']\n            self.convs.append(conv_map[config['conv_type']](in_dim, out_dim))\n            in_dim = out_dim\n        self.activation = F.relu if config['activation'] == 'relu' else F.elu\n        self.dropout = config['dropout']\n        self.classifier = nn.Linear(2*in_dim, 2)\n    def forward(self, x, edge_index, target_edges=None):\n        for conv in self.convs:\n            x = conv(x, edge_index)\n            x = self.activation(x)\n            x = F.dropout(x, self.dropout, training=self.training)\n        if target_edges is None:\n            target_edges = edge_index\n        src_x = x[target_edges[0]]\n        dst_x = x[target_edges[1]]\n        edge_feat = torch.cat([src_x, dst_x], dim=-1)\n        return self.classifier(edge_feat)\n\nclass CarbonAwareEdgeNAS:\n    def __init__(self, data, population_size=8, generations=3, seed=42, max_attempts=80,\n                 proxy_acc_threshold=0.7, proxy_epochs=20):\n        set_seed(seed)\n        self.data = data\n        self.population_size = population_size\n        self.generations = generations\n        self.search_space = GNNSearchSpace()\n        self.max_attempts = max_attempts\n        self.proxy_acc_threshold = proxy_acc_threshold\n        self.proxy_epochs = proxy_epochs\n        self.results = []\n\n    def proxy_train_and_eval(self, config, max_epochs=None, acc_threshold=None):\n        if max_epochs is None:\n            max_epochs = self.proxy_epochs\n        if acc_threshold is None:\n            acc_threshold = self.proxy_acc_threshold\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model = EdgeScoreModel(config, self.data.x.size(1)).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n        criterion = nn.CrossEntropyLoss()\n        tracker = EmissionsTracker(\n            project_name=f\"epinions_edge_nas_proxy\",\n            measure_power_secs=10,\n            output_file=\"emissions_epinions.csv\",\n            log_level='error'\n        )\n        tracker.start()\n        x, edge_index = self.data.x.to(device), self.data.edge_index.to(device)\n        edge_y = self.data.edge_y.to(device)\n        train_mask = self.data.train_edge_mask\n        val_mask = self.data.val_edge_mask\n        best_val_acc = 0.0\n        for epoch in range(max_epochs):\n            model.train()\n            optimizer.zero_grad()\n            out = model(x, edge_index, edge_index[:, train_mask])\n            loss = criterion(out, edge_y[train_mask])\n            loss.backward()\n            optimizer.step()\n            model.eval()\n            with torch.no_grad():\n                logits = model(x, edge_index, edge_index[:, val_mask])\n                pred = logits.argmax(dim=1)\n                acc = (pred == edge_y[val_mask]).float().mean().item()\n                best_val_acc = max(best_val_acc, acc)\n            if best_val_acc >= acc_threshold:\n                break\n        emissions = tracker.stop()\n        return {'val_acc': best_val_acc, 'carbon': emissions, 'model': model}\n\n    def train_and_evaluate(self, config, epochs=40):\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model = EdgeScoreModel(config, self.data.x.size(1)).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n        criterion = nn.CrossEntropyLoss()\n        x, edge_index = self.data.x.to(device), self.data.edge_index.to(device)\n        edge_y = self.data.edge_y.to(device)\n        train_mask = self.data.train_edge_mask\n        val_mask = self.data.val_edge_mask\n        for epoch in range(epochs):\n            model.train()\n            optimizer.zero_grad()\n            out = model(x, edge_index, edge_index[:, train_mask])\n            loss = criterion(out, edge_y[train_mask])\n            loss.backward()\n            optimizer.step()\n        model.eval()\n        with torch.no_grad():\n            logits = model(x, edge_index, edge_index[:, val_mask])\n            pred = logits.argmax(dim=1)\n            acc = (pred == edge_y[val_mask]).float().mean().item()\n        return {'val_acc': acc, 'model': model}\n\n    def run_search(self):\n        print(\"Starting Carbon-Aware Edge NAS on Epinions...\")\n        baseline_config = {'conv_type': 'GCN', 'hidden_dim': 64, 'num_layers': 2,\n                           'activation': 'relu', 'dropout': 0.5}\n        baseline_result = self.train_and_evaluate(baseline_config, epochs=40)\n        self.best_score = baseline_result['val_acc']\n        self.best_architecture = baseline_config\n        print(\"Baseline - Val Accuracy:\", round(self.best_score, 4))\n        for generation in range(self.generations):\n            population = []\n            attempts = 0\n            while len(population) < self.population_size and attempts < self.max_attempts:\n                config = self.search_space.sample_architecture()\n                result = self.proxy_train_and_eval(config)\n                if result['val_acc'] >= self.proxy_acc_threshold:\n                    population.append({'config': config, 'val_acc': result['val_acc'], 'carbon': result['carbon']})\n                    print(f\"[Gen {generation+1}] Found: Proxy Val Acc={result['val_acc']:.4f}\")\n                attempts += 1\n            if not population:\n                print(f\"[Gen {generation+1}] No good candidates.\")\n                continue\n            final_population = []\n            for cand in population:\n                full_res = self.train_and_evaluate(cand['config'], epochs=40)\n                final_population.append({'config': cand['config'], 'val_acc': full_res['val_acc']})\n            final_population.sort(key=lambda x: x['val_acc'], reverse=True)\n            best = final_population[0]\n            if best['val_acc'] > self.best_score:\n                self.best_score = best['val_acc']\n                self.best_architecture = best['config']\n\n        print(\"Retraining best NAS architecture on all edge splits for test evaluation...\")\n        # Train on train+val, test on test split\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model = EdgeScoreModel(self.best_architecture, self.data.x.size(1)).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n        criterion = nn.CrossEntropyLoss()\n        x, edge_index = self.data.x.to(device), self.data.edge_index.to(device)\n        edge_y = self.data.edge_y.to(device)\n        full_mask = self.data.train_edge_mask | self.data.val_edge_mask\n        test_mask = self.data.test_edge_mask\n        for epoch in range(40):\n            model.train()\n            optimizer.zero_grad()\n            out = model(x, edge_index, edge_index[:, full_mask])\n            loss = criterion(out, edge_y[full_mask])\n            loss.backward()\n            optimizer.step()\n        model.eval()\n        with torch.no_grad():\n            logits = model(x, edge_index, edge_index[:, test_mask])\n            pred = logits.argmax(dim=1).cpu().numpy()\n            test_labels = edge_y[test_mask].cpu().numpy()\n            test_acc = (pred == test_labels).mean()\n            test_f1 = f1_score(test_labels, pred, average='macro')\n        print(\"--- Final Results ---\")\n        print(\"Best Architecture:\", self.best_architecture)\n        print(\"Test Accuracy:\", round(test_acc, 4))\n        print(\"Test Macro-F1:\", round(test_f1, 4))\n\n# ---- Usage ----\nif __name__ == '__main__':\n    set_seed(42)\n    G, labels = load_epinions_graph('soc-Epinions1.txt')\n    data = pyg_data_from_nx(G, labels)\n    data = create_edge_splits(data, seed=42)\n    nas = CarbonAwareEdgeNAS(\n        data, \n        population_size=8, \n        generations=3,\n        max_attempts=80,\n        proxy_acc_threshold=0.7,\n        proxy_epochs=20\n    )\n    nas.run_search()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:37:29.082556Z","iopub.execute_input":"2025-09-22T14:37:29.082867Z","iopub.status.idle":"2025-09-22T14:37:29.310540Z","shell.execute_reply.started":"2025-09-22T14:37:29.082840Z","shell.execute_reply":"2025-09-22T14:37:29.309611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport urllib.request\nimport gzip\nimport shutil\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport random\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score, f1_score\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GCNConv, SAGEConv, GATConv\nfrom codecarbon import EmissionsTracker\n\ndef set_seed(seed=42):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n# --- AUTOMATIC DOWNLOAD for WikiVote ---\nurl = \"https://snap.stanford.edu/data/wiki-Vote.txt.gz\"\nlocal_gz = \"wiki-Vote.txt.gz\"\nlocal_txt = \"wiki-Vote.txt\"\nif not os.path.exists(local_txt):\n    print(\"Downloading wiki-Vote.txt.gz...\")\n    urllib.request.urlretrieve(url, local_gz)\n    print(\"Extracting...\")\n    with gzip.open(local_gz, 'rb') as f_in:\n        with open(local_txt, 'wb') as f_out:\n            shutil.copyfileobj(f_in, f_out)\n    print(\"File ready:\", local_txt)\nelse:\n    print(\"File already exists:\", local_txt)\n\ndef load_wikivote_edges(path):\n    edge_list = []\n    nodes = set()\n    with open(path, 'r') as f:\n        for line in f:\n            if line.startswith('#') or not line.strip():\n                continue\n            tokens = line.strip().split()\n            if len(tokens) != 2:\n                continue\n            u, v = map(int, tokens)\n            edge_list.append((u, v))\n            nodes.update([u, v])\n    nodes = sorted(list(nodes))\n    node_map = {n: i for i, n in enumerate(nodes)}\n    pos_edges = [(node_map[u], node_map[v]) for (u, v) in edge_list]\n    return pos_edges, nodes, node_map\n\ndef sample_negative_edges(pos_edges, num_nodes, num_negs, exclude_set=None, seed=42):\n    rng = np.random.RandomState(seed)\n    neg_edges = set()\n    pos_set = set(pos_edges)\n    if exclude_set is None:\n        exclude_set = pos_set\n    while len(neg_edges) < num_negs:\n        idx_s = rng.randint(0, num_nodes)\n        idx_t = rng.randint(0, num_nodes)\n        if idx_s == idx_t:\n            continue\n        candidate = (idx_s, idx_t)\n        if candidate in exclude_set or candidate in neg_edges:\n            continue\n        neg_edges.add(candidate)\n    return list(neg_edges)\n\ndef build_link_data(pos_edges, neg_edges, num_nodes):\n    # Label 1 for pos, 0 for neg\n    all_edges = pos_edges + neg_edges\n    edge_label = [1]*len(pos_edges) + [0]*len(neg_edges)\n    edge_index = torch.tensor(all_edges, dtype=torch.long).t().contiguous()\n    edge_label = torch.tensor(edge_label, dtype=torch.float)\n    x = torch.ones((num_nodes, 8))  # dummy node features\n    return Data(x=x, edge_index=edge_index, edge_label=edge_label)\n\ndef create_edge_split(data, seed=42):\n    np.random.seed(seed)\n    num_edges = data.edge_index.size(1)\n    perm = np.random.permutation(num_edges)\n    num_train = int(0.7 * num_edges)\n    num_val = int(0.1 * num_edges)\n    train_idx = perm[:num_train]\n    val_idx = perm[num_train:num_train+num_val]\n    test_idx = perm[num_train+num_val:]\n    data.train_edge_mask = torch.zeros(num_edges, dtype=torch.bool)\n    data.val_edge_mask = torch.zeros(num_edges, dtype=torch.bool)\n    data.test_edge_mask = torch.zeros(num_edges, dtype=torch.bool)\n    data.train_edge_mask[train_idx] = True\n    data.val_edge_mask[val_idx] = True\n    data.test_edge_mask[test_idx] = True\n    return data\n\nclass GNNSearchSpace:\n    def __init__(self):\n        self.conv_types = ['GCN', 'SAGE', 'GAT']\n        self.hidden_dims = [32, 64, 128]\n        self.num_layers = [2, 3]\n        self.activation_types = ['relu', 'elu']\n        self.dropout_rates = [0.2, 0.5]\n    def sample_architecture(self):\n        return {\n            'conv_type': random.choice(self.conv_types),\n            'hidden_dim': random.choice(self.hidden_dims),\n            'num_layers': random.choice(self.num_layers),\n            'activation': random.choice(self.activation_types),\n            'dropout': random.choice(self.dropout_rates),\n        }\n\nclass EdgeScoreModel(nn.Module):\n    def __init__(self, config, input_dim):\n        super().__init__()\n        conv_map = {'GCN': GCNConv, 'SAGE': SAGEConv, 'GAT': GATConv}\n        self.convs = nn.ModuleList()\n        in_dim = input_dim\n        for i in range(config['num_layers']):\n            out_dim = config['hidden_dim']\n            self.convs.append(conv_map[config['conv_type']](in_dim, out_dim))\n            in_dim = out_dim\n        self.activation = F.relu if config['activation'] == 'relu' else F.elu\n        self.dropout = config['dropout']\n        self.classifier = nn.Linear(2*in_dim, 1)\n    def forward(self, x, edge_index, target_edges=None):\n        for conv in self.convs:\n            x = conv(x, edge_index)\n            x = self.activation(x)\n            x = F.dropout(x, self.dropout, training=self.training)\n        if target_edges is None:\n            target_edges = edge_index\n        src_x = x[target_edges[0]]\n        dst_x = x[target_edges[1]]\n        edge_feat = torch.cat([src_x, dst_x], dim=-1)\n        return self.classifier(edge_feat).squeeze(-1)\n\nclass CarbonAwareEdgeNAS:\n    def __init__(self, data, population_size=8, generations=3, seed=42, max_attempts=80,\n                 proxy_auc_threshold=0.7, proxy_epochs=20):\n        set_seed(seed)\n        self.data = data\n        self.population_size = population_size\n        self.generations = generations\n        self.search_space = GNNSearchSpace()\n        self.max_attempts = max_attempts\n        self.proxy_auc_threshold = proxy_auc_threshold\n        self.proxy_epochs = proxy_epochs\n        self.results = []\n\n    def proxy_train_and_eval(self, config, max_epochs=None, auc_threshold=None):\n        if max_epochs is None:\n            max_epochs = self.proxy_epochs\n        if auc_threshold is None:\n            auc_threshold = self.proxy_auc_threshold\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model = EdgeScoreModel(config, self.data.x.size(1)).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n        criterion = nn.BCEWithLogitsLoss()\n        tracker = EmissionsTracker(\n            project_name=f\"wikivote_edge_nas_proxy\",\n            measure_power_secs=10,\n            output_file=\"emissions_wikivote.csv\",\n            log_level='error'\n        )\n        tracker.start()\n        x, edge_index = self.data.x.to(device), self.data.edge_index.to(device)\n        edge_label = self.data.edge_label.to(device)\n        train_mask = self.data.train_edge_mask\n        val_mask = self.data.val_edge_mask\n        best_val_auc = 0.0\n        for epoch in range(max_epochs):\n            model.train()\n            optimizer.zero_grad()\n            out = model(x, edge_index, edge_index[:, train_mask])\n            loss = criterion(out, edge_label[train_mask])\n            loss.backward()\n            optimizer.step()\n            model.eval()\n            with torch.no_grad():\n                logits = model(x, edge_index, edge_index[:, val_mask])\n                probs = torch.sigmoid(logits).cpu().numpy()\n                gt = edge_label[val_mask].cpu().numpy()\n                auc = roc_auc_score(gt, probs)\n                best_val_auc = max(best_val_auc, auc)\n            if best_val_auc >= auc_threshold:\n                break\n        emissions = tracker.stop()\n        return {'val_auc': best_val_auc, 'carbon': emissions, 'model': model}\n\n    def train_and_evaluate(self, config, epochs=40):\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model = EdgeScoreModel(config, self.data.x.size(1)).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n        criterion = nn.BCEWithLogitsLoss()\n        x, edge_index = self.data.x.to(device), self.data.edge_index.to(device)\n        edge_label = self.data.edge_label.to(device)\n        train_mask = self.data.train_edge_mask\n        val_mask = self.data.val_edge_mask\n        for epoch in range(epochs):\n            model.train()\n            optimizer.zero_grad()\n            out = model(x, edge_index, edge_index[:, train_mask])\n            loss = criterion(out, edge_label[train_mask])\n            loss.backward()\n            optimizer.step()\n        model.eval()\n        with torch.no_grad():\n            logits = model(x, edge_index, edge_index[:, val_mask])\n            probs = torch.sigmoid(logits).cpu().numpy()\n            gt = edge_label[val_mask].cpu().numpy()\n            auc = roc_auc_score(gt, probs)\n        return {'val_auc': auc, 'model': model}\n\n    def run_search(self):\n        print(\"Starting Carbon-Aware Edge NAS on WikiVote...\")\n        baseline_config = {'conv_type': 'GCN', 'hidden_dim': 64, 'num_layers': 2,\n                           'activation': 'relu', 'dropout': 0.5}\n        baseline_result = self.train_and_evaluate(baseline_config, epochs=40)\n        self.best_score = baseline_result['val_auc']\n        self.best_architecture = baseline_config\n        print(\"Baseline - Val AUC:\", round(self.best_score, 4))\n        for generation in range(self.generations):\n            population = []\n            attempts = 0\n            while len(population) < self.population_size and attempts < self.max_attempts:\n                config = self.search_space.sample_architecture()\n                result = self.proxy_train_and_eval(config)\n                if result['val_auc'] >= self.proxy_auc_threshold:\n                    population.append({'config': config, 'val_auc': result['val_auc'], 'carbon': result['carbon']})\n                    print(f\"[Gen {generation+1}] Found: Proxy Val AUC={result['val_auc']:.4f}\")\n                attempts += 1\n            if not population:\n                print(f\"[Gen {generation+1}] No good candidates.\")\n                continue\n            final_population = []\n            for cand in population:\n                full_res = self.train_and_evaluate(cand['config'], epochs=40)\n                final_population.append({'config': cand['config'], 'val_auc': full_res['val_auc']})\n            final_population.sort(key=lambda x: x['val_auc'], reverse=True)\n            best = final_population[0]\n            if best['val_auc'] > self.best_score:\n                self.best_score = best['val_auc']\n                self.best_architecture = best['config']\n\n        print(\"Retraining best NAS architecture on all edge splits for test evaluation...\")\n        # Train on train+val, test on test split\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model = EdgeScoreModel(self.best_architecture, self.data.x.size(1)).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n        criterion = nn.BCEWithLogitsLoss()\n        x, edge_index = self.data.x.to(device), self.data.edge_index.to(device)\n        edge_label = self.data.edge_label.to(device)\n        full_mask = self.data.train_edge_mask | self.data.val_edge_mask\n        test_mask = self.data.test_edge_mask\n        for epoch in range(40):\n            model.train()\n            optimizer.zero_grad()\n            out = model(x, edge_index, edge_index[:, full_mask])\n            loss = criterion(out, edge_label[full_mask])\n            loss.backward()\n            optimizer.step()\n        model.eval()\n        with torch.no_grad():\n            logits = model(x, edge_index, edge_index[:, test_mask])\n            probs = torch.sigmoid(logits).cpu().numpy()\n            gt = edge_label[test_mask].cpu().numpy()\n            auc = roc_auc_score(gt, probs)\n            pred_label = (probs > 0.5).astype(np.int64)\n            test_f1 = f1_score(gt, pred_label, average='macro')\n        print(\"--- Final Results ---\")\n        print(\"Best Architecture:\", self.best_architecture)\n        print(\"Test AUC:\", round(auc, 4))\n        print(\"Test Macro-F1:\", round(test_f1, 4))\n\n# ---- Usage ----\nif __name__ == '__main__':\n    set_seed(42)\n    pos_edges, nodes, node_map = load_wikivote_edges('wiki-Vote.txt')\n    num_pos = len(pos_edges)\n    num_nodes = len(nodes)\n    neg_edges = sample_negative_edges(pos_edges, num_nodes, num_pos, exclude_set=set(pos_edges), seed=42)\n    data = build_link_data(pos_edges, neg_edges, num_nodes)\n    data = create_edge_split(data, seed=42)\n    nas = CarbonAwareEdgeNAS(\n        data, \n        population_size=8, \n        generations=3,\n        max_attempts=80,\n        proxy_auc_threshold=0.7,\n        proxy_epochs=20\n    )\n    nas.run_search()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T15:09:40.006461Z","iopub.execute_input":"2025-09-22T15:09:40.007017Z","iopub.status.idle":"2025-09-22T15:12:20.924454Z","shell.execute_reply.started":"2025-09-22T15:09:40.006994Z","shell.execute_reply":"2025-09-22T15:12:20.923829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport random\nimport copy\nimport pandas as pd\nfrom torch_geometric.datasets import Amazon\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GCNConv, GATConv, SAGEConv\nfrom torch_geometric.nn import BatchNorm, LayerNorm\nfrom codecarbon import EmissionsTracker\n\ndef set_seed(seed=42):\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nclass GNNSearchSpace:\n    def __init__(self):\n        self.conv_types = ['GCN', 'GAT', 'SAGE']\n        self.norm_types = ['batch', 'layer', 'none']\n        self.activation_types = ['relu', 'elu']\n        self.hidden_dims = [64, 128, 256]\n        self.num_layers = [2, 3]\n        self.dropout_rates = [0.5]\n    def sample_architecture(self):\n        return {\n            'conv_type': random.choice(self.conv_types),\n            'activation': random.choice(self.activation_types),\n            'norm_type': random.choice(self.norm_types),\n            'hidden_dim': random.choice(self.hidden_dims),\n            'num_layers': random.choice(self.num_layers),\n            'dropout': random.choice(self.dropout_rates)\n        }\n\nclass GNNModel(nn.Module):\n    def __init__(self, config, input_dim, output_dim):\n        super().__init__()\n        self.num_layers = config['num_layers']\n        self.config = config\n        self.convs = nn.ModuleList()\n        self.norms = nn.ModuleList()\n        in_dim = input_dim\n        hidden_dim = config['hidden_dim']\n        for i in range(self.num_layers):\n            out_dim = hidden_dim if i < self.num_layers - 1 else output_dim\n            if config['conv_type'] == 'GCN':\n                conv = GCNConv(in_dim, out_dim)\n            elif config['conv_type'] == 'GAT':\n                conv = GATConv(in_dim, out_dim, heads=1)\n            elif config['conv_type'] == 'SAGE':\n                conv = SAGEConv(in_dim, out_dim)\n            self.convs.append(conv)\n            if i < self.num_layers - 1:\n                if config['norm_type'] == 'batch':\n                    norm = BatchNorm(out_dim)\n                elif config['norm_type'] == 'layer':\n                    norm = LayerNorm(out_dim)\n                else:\n                    norm = nn.Identity()\n                self.norms.append(norm)\n            in_dim = out_dim\n\n    def forward(self, x, edge_index):\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n            if i < self.num_layers - 1:\n                x = self.norms[i](x)\n                if self.config['activation'] == 'relu':\n                    x = F.relu(x)\n                elif self.config['activation'] == 'elu':\n                    x = F.elu(x)\n                x = F.dropout(x, p=self.config['dropout'], training=self.training)\n        return x\n\nclass CarbonAwareGNNNAS:\n    def __init__(self, dataset_name='Photo', population_size=12, generations=5, seed=42,\n                 max_attempts=200, proxy_acc_threshold=0.90, max_proxy_epochs_first_gen=80):\n        set_seed(seed)\n        self.dataset_name = dataset_name\n        self.population_size = population_size\n        self.generations = generations\n        self.search_space = GNNSearchSpace()\n        self.output_file = f\"{self.dataset_name}_emissions.csv\"\n        self.max_attempts = max_attempts\n        self.proxy_acc_threshold = proxy_acc_threshold\n        self.max_proxy_epochs_first_gen = max_proxy_epochs_first_gen\n        self.load_dataset()\n        self.results = []\n        self.best_architecture = None\n        self.best_score = 0.0\n        self.best_model = None\n\n    def load_dataset(self):\n            dataset = Amazon(root=f'data/{self.dataset_name}', name=self.dataset_name)\n            data = dataset[0]\n            self.input_dim = dataset.num_node_features\n            self.output_dim = dataset.num_classes\n            N = data.y.size(0)\n            indices = np.arange(N)\n            np.random.shuffle(indices)\n            train_size = int(0.6 * N)\n            val_size = int(0.2 * N)\n            train_idx = torch.tensor(indices[:train_size])\n            val_idx = torch.tensor(indices[train_size:train_size + val_size])\n            test_idx = torch.tensor(indices[train_size + val_size:])\n    # Build masks\n            data.train_mask = torch.zeros(N, dtype=torch.bool)\n            data.val_mask = torch.zeros(N, dtype=torch.bool)\n            data.test_mask = torch.zeros(N, dtype=torch.bool)\n            data.train_mask[train_idx] = True\n            data.val_mask[val_idx] = True\n            data.test_mask[test_idx] = True\n            self.data = data\n            print(f\"Dataset: Amazon {self.dataset_name} | Features: {self.input_dim} | Classes: {self.output_dim} | Nodes: {N}\")\n\n\n\n    def calculate_block_reuse(self, arch1, arch2):\n        reuse_score = 0.0\n        total = len(arch1)\n        for k in arch1.keys():\n            if arch1[k] == arch2[k]:\n                reuse_score += 1\n        return reuse_score / total\n\n    def estimate_proxy_carbon(self, config):\n        layer_weights = {'GCN': 1.0, 'GAT': 1.5, 'SAGE': 1.2}\n        norm_weights = {'batch': 0.2, 'layer': 0.1, 'none': 0.0}\n        return (layer_weights[config['conv_type']] +\n                config['hidden_dim'] / 128.0 +\n                config['num_layers'] / 3.0 +\n                norm_weights[config['norm_type']])\n\n    def proxy_train_and_eval(self, config, max_epochs=None, acc_threshold=None, check_every=5, min_epochs=10):\n        if max_epochs is None:\n            max_epochs = self.max_proxy_epochs_first_gen\n        if acc_threshold is None:\n            acc_threshold = self.proxy_acc_threshold\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model = GNNModel(config, self.input_dim, self.output_dim).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n        criterion = nn.CrossEntropyLoss()\n        tracker = EmissionsTracker(\n            project_name=f\"gnn_nas_proxy_amazon_{self.dataset_name}\",\n            measure_power_secs=15,\n            output_file=self.output_file\n        )\n        tracker.start()\n        best_val_acc = 0.0\n        data = self.data.to(device)\n        for epoch in range(max_epochs):\n            model.train()\n            out = model(data.x, data.edge_index)\n            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            model.eval()\n            logits = model(data.x, data.edge_index)\n            val_pred = logits[data.val_mask].argmax(dim=1)\n            val_acc = (val_pred == data.y[data.val_mask]).float().mean().item()\n            best_val_acc = max(best_val_acc, val_acc)\n            if epoch >= min_epochs and (best_val_acc >= acc_threshold):\n                break\n        emissions = tracker.stop()\n        carbon = emissions[\"emissions\"] if emissions and isinstance(emissions, dict) and \"emissions\" in emissions else (self.estimate_proxy_carbon(config) * 0.01)\n        return {'accuracy': best_val_acc, 'carbon': carbon, 'model': model}\n\n    def train_and_evaluate(self, config, epochs=200):\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model = GNNModel(config, self.input_dim, self.output_dim).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n        criterion = nn.CrossEntropyLoss()\n        tracker = EmissionsTracker(\n            project_name=f\"gnn_nas_amazon_{self.dataset_name}\",\n            measure_power_secs=15,\n            output_file=self.output_file\n        )\n        tracker.start()\n        data = self.data.to(device)\n        for epoch in range(epochs):\n            model.train()\n            out = model(data.x, data.edge_index)\n            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        model.eval()\n        logits = model(data.x, data.edge_index)\n        val_pred = logits[data.val_mask].argmax(dim=1)\n        val_acc = (val_pred == data.y[data.val_mask]).float().mean().item()\n        emissions = tracker.stop()\n        carbon = emissions[\"emissions\"] if emissions and isinstance(emissions, dict) and \"emissions\" in emissions else (self.estimate_proxy_carbon(config) * 0.01)\n        return {'accuracy': val_acc, 'carbon': carbon, 'model': model}\n\n    def mutate_architecture(self, base_config):\n        # Key mapping to avoid KeyError\n        component_map = {\n            'conv_types': 'conv_type',\n            'activation_types': 'activation',\n            'norm_types': 'norm_type',\n            'hidden_dims': 'hidden_dim',\n            'num_layers': 'num_layers',\n            'dropout_rates': 'dropout'\n        }\n        if base_config is None: return self.search_space.sample_architecture()\n        new_config = copy.deepcopy(base_config)\n        component_to_mutate = random.choice(list(component_map.keys()))\n        config_key = component_map[component_to_mutate]\n        current_value = new_config[config_key]\n        options = getattr(self.search_space, component_to_mutate)\n        possible_values = [v for v in options if v != current_value]\n        if possible_values:\n            new_config[config_key] = random.choice(possible_values)\n        return new_config\n\n    def run_search(self):\n        print(f\"Starting Carbon-Aware GNN NAS on Amazon {self.dataset_name}...\")\n        self.baseline_config = {\n            'conv_type': 'GCN', 'activation': 'relu', 'norm_type': 'batch',\n            'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.5\n        }\n        print(\"Training baseline...\")\n        baseline_result = self.train_and_evaluate(self.baseline_config, epochs=200)\n        print(f\"Baseline - Val Accuracy: {baseline_result['accuracy']:.4f}, Carbon: {baseline_result['carbon']}\")\n        max_proxy_epochs_first_gen = 80\n        max_proxy_epochs_others = 40\n        proxy_acc_threshold = self.proxy_acc_threshold\n        full_training_epochs = 200\n        final_retrain_epochs = 300\n\n        for generation in range(self.generations):\n            print(f\"\\nGeneration {generation + 1}/{self.generations}\")\n            current_population = []\n            if generation == 0:\n                print(\"  Sampling initial viable population...\")\n                attempts = 0\n                while len(current_population) < self.population_size and attempts < self.max_attempts:\n                    config = self.search_space.sample_architecture()\n                    result = self.proxy_train_and_eval(config, max_proxy_epochs_first_gen, proxy_acc_threshold)\n                    if result['accuracy'] >= proxy_acc_threshold:\n                        current_population.append({'config': config, 'proxy_acc': result['accuracy']})\n                        print(f\"    Found candidate {len(current_population)} with Proxy Acc={result['accuracy']:.4f}\")\n                    attempts += 1\n                if len(current_population) < self.population_size:\n                    print(f\"  Warning: Only {len(current_population)} viable candidates found after {attempts} attempts!\")\n            else:\n                print(\"  Mutating best architecture to create new population...\")\n                while len(current_population) < self.population_size:\n                    config = self.mutate_architecture(self.best_architecture)\n                    result = self.proxy_train_and_eval(config, max_proxy_epochs_others, proxy_acc_threshold)\n                    if result['accuracy'] >= proxy_acc_threshold:\n                        current_population.append({'config': config, 'proxy_acc': result['accuracy']})\n                        print(f\"    Found mutated candidate {len(current_population)} with Proxy Acc={result['accuracy']:.4f}\")\n\n            evaluated_population = []\n            for candidate in current_population:\n                full_result = self.train_and_evaluate(candidate['config'], epochs=full_training_epochs)\n                reuse_score = self.calculate_block_reuse(candidate['config'], self.baseline_config)\n                evaluated_population.append({\n                    'config': candidate['config'],\n                    'accuracy': full_result['accuracy'],\n                    'carbon': full_result['carbon'],\n                    'reuse': reuse_score,\n                    'model': full_result['model']\n                })\n                print(f\"    Candidate: Acc={full_result['accuracy']:.4f}, Carbon={full_result['carbon']:.6f}, Reuse={reuse_score:.3f}\")\n            if evaluated_population:\n                for cand in evaluated_population:\n                    cand['score'] = (cand['accuracy'] * 0.6\n                                    + (1 - min(cand['carbon'], 0.1) / 0.1) * 0.2\n                                    + cand['reuse'] * 0.2)\n                evaluated_population.sort(key=lambda x: x['score'], reverse=True)\n                best_of_gen = evaluated_population[0]\n                if best_of_gen['score'] > self.best_score:\n                    self.best_score = best_of_gen['score']\n                    self.best_architecture = best_of_gen['config']\n                    self.best_model = best_of_gen['model']\n                    print(f\"  New best architecture found! Score: {self.best_score:.4f}\")\n        if not self.best_architecture:\n            print(\"No suitable architecture found. Exiting.\")\n            return\n        print(\"\\nRetraining best architecture on full data...\")\n        final_result = self.train_and_evaluate(self.best_architecture, epochs=final_retrain_epochs)\n        data = self.data\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.best_model.eval()\n        logits = final_result['model'](data.x.to(device), data.edge_index.to(device))\n        test_pred = logits[data.test_mask].argmax(dim=1)\n        test_acc = (test_pred == data.y[data.test_mask].to(device)).float().mean().item()\n        print(\"\\n--- Final Results ---\")\n        print(f\"Best Architecture: {self.best_architecture}\")\n        print(f\"Final Test Accuracy: {test_acc:.4f}\")\n        print(f\"Final Carbon Footprint: {final_result['carbon']:.6f} kg CO2\")\n        print(f\"Block Reuse Score: {self.calculate_block_reuse(self.best_architecture, self.baseline_config):.3f}\")\n        results_df = pd.DataFrame([{\n            'dataset': self.dataset_name,\n            'best_config': str(self.best_architecture),\n            'test_accuracy': test_acc,\n            'carbon_footprint': final_result['carbon'],\n            'reuse_score': self.calculate_block_reuse(self.best_architecture, self.baseline_config)\n        }])\n        results_df.to_csv(f'{self.dataset_name.lower()}_gnn_nas_results.csv', index=False)\n\nif __name__ == \"__main__\":\n    nas_runner = CarbonAwareGNNNAS(dataset_name='Photo', proxy_acc_threshold=0.90)\n    nas_runner.run_search()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:15:48.622449Z","iopub.execute_input":"2025-09-22T16:15:48.622916Z","iopub.status.idle":"2025-09-22T16:29:35.209014Z","shell.execute_reply.started":"2025-09-22T16:15:48.622893Z","shell.execute_reply":"2025-09-22T16:29:35.208478Z"}},"outputs":[],"execution_count":null}]}